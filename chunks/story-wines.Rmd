##  {.emphasizedabit}
A startup wants you to predict wine quality from its chemical composition.

# Let's predict wine quality

## Step 1: Find data

Free online dataset about red and white variants of the Portuguese "Vinho Verde" wine.

Target: Quality from 1 to 10
Features: Acidity, alcohol ...

<font size="2">P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.
Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.</font>

## Step 1: Find data

- fixed acidity
- volatile acidity
- citric acid
- residual sugar
- chlorides
- free sulfur dioxide
- total sulfur dioxide
- density
- pH
- sulphates
- alcohol
- quality based on sensory data (0-10)

<!-- Draw: -->
<!-- Volatile acidity as polish remover, pH as pH paper, alchohol as liqour bottle, chlorides as salt prinkler, citric acid as lemon,  -->
<!-- sulfur as mushroom?,  -->

## Step 1: Find data


```{r load-data,include=FALSE}
library('mlr')
library('ggplot2')
library('tidyr')
library('lime')
devtools::install_github("christophM/iml", ref = "feature-effects")
library('iml')
source('../code/prepare-wine-data.R')
```

```{r show-dist}
ggplot(wine) + 
  geom_bar(aes(x = quality)) + 
  scale_x_continuous("Wine Quality", labels = 1:10, breaks = 1:10)
```

# Step 2: Throw ML on your data

##

10x CV
Test linear regression model, decision tree, random forest


## {.center data-background=../images/comp-dog.gif data-background-size=contain}


## Step 2: Throw ML on your data

```{r benchmark}
library("mlr")
lrn.ranger = makeLearner("regr.ranger")
lrn.lm = makeLearner("regr.lm")
lrn.rpart = makeLearner("regr.rpart")

rdesc = cv5

lrns = list(lrn.ranger, lrn.lm, lrn.rpart)

tsk = makeRegrTask(data = wine, target = "quality")

bmr = benchmark(lrns, tsk, rdesc, measures = list(mae))
bmr_tab = getBMRAggrPerformances(bmr, as.df = TRUE)
knitr::kable(bmr_tab[-1])
```

=> The random forest (ranger) is the best model.

```{r final-model}
mod = train(lrn.ranger, tsk)

set.seed(42)
sample_size = 500
wine_subset = wine[sample(1:nrow(wine), size = sample_size),]

pred = Predictor$new(mod, data = wine_subset, y = "quality")
```


## Step 2: Throw ML on your data

```{r check-model}

preds = pred$predict(wine)
preds$actual = wine$quality
ggplot(preds, aes(x = actual, y = .prediction, group = actual)) + 
  geom_violin(aes(fill = ..n..)) +
  scale_x_continuous("Actual quality (jittered)", 
    labels = 1:10, breaks = 1:10) + 
  scale_y_continuous("Predicted quality", labels = 1:10, breaks = 1:10) + 
  scale_fill_gradient(low = "white", high = "darkblue", guide = "none")
```


## {data-background=../images/delivery.gif data-background-size=contain}


# Step 3: Profit {.center}

## {data-background=../images/done-here.gif data-background-size=contain}
```{r, echo=FALSE, out.width='80%', include = FALSE}
knitr::include_graphics("../images/done-here.gif" )
```

##  {.emphasizedabit}

Client: "We would love to learn some insights."



##  {.emphasizedabit .center data-background=../images/black-box.gif data-background-size=cover}

<div class="white">
Looking inside the black box
</div>

## What are the most important features?

```{r feature-importance}
imp = FeatureImp$new(pred, loss = "mae")
plot(imp)
```

## How do features affect predictions?

```{r}
effs = FeatureEffects$new(pred)
plot(effs, ncols = 4)
```

## How do features affect predictions?
Method: Partial dependence plot, individual conditional expectation curves
```{r}
eff = FeatureEffect$new(pred, "alcohol", method = 'pdp+ice')
plot(eff)
```

## Interactions with type of wine?
Method: H-statistic for interactions

```{r}
inter = Interaction$new(pred)
plot(inter)
```

## Interactions of alcohol?
Method: H-statistic for interactions

```{r}
inter.a = Interaction$new(pred, "alcohol")
plot(inter.a)
```


## Rule of thumb for wine quality?
Method: Surrogate Model

```{r surrogate}
tree = TreeSurrogate$new(pred, maxdepth  = 2)
plot(tree$tree)
```


## Exceptionally bad wine

A customer gets a really bad prediction. What was the reason?

```{r bad-wine}
# find bad wine in data
predictions = pred$predict(wine)

min_pred = min(predictions)
worst = wine[which(predictions == min_pred),]

# explain with shapley
knitr::kable(t(worst))
# explain with cf?

min_pred

```

## Shapley Value

```{r}
shap = Shapley$new(pred, x.interest = worst)
plot(shap)
```

## The best wine

```{r best-wine}
# find bad wine in data
max_pred = max(predictions)
best = wine[which(predictions == max_pred),]

# explain with shapley
knitr::kable(t(best))
# explain with cf?

max_pred

```

## The best wine
Technique: Shapley Values

```{r}
shap = Shapley$new(pred, x.interest = best)
plot(shap)
```


