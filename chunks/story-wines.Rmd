TODO: Add wine csv and R script


##  {.emphasizedabit}
A startup wants you to predict wine quality from its chemical composition.

# Let's predict wine quality


# What is Machine Learning?

<div class="notes">
Machine learning is a method for teaching computers to make and improve predictions or behaviours based on data.
</div>

## {data-background=../images/magic.jpg data-background-size=contain}


## Step 1: Find data

Red and white variants of the Portuguese "Vinho Verde" wine.

Target: Quality from 1 to 10
Features: Acidity, alcohol ...

<font size="2">P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.
Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.</font>

## Step 1: Find data


TODO: Draw image with features.

1 - fixed acidity
2 - volatile acidity
3 - citric acid
4 - residual sugar
5 - chlorides
6 - free sulfur dioxide
7 - total sulfur dioxide
8 - density
9 - pH
10 - sulphates
11 - alcohol
Output variable (based on sensory data):
12 - quality (score between 0 and 10)

Draw:
Volatile acidity as polish remover, pH as pH paper, alchohol as liqour bottle, chlorides as salt prinkler, citric acid as lemon, 
sulfur as mushroom?, 

## Step 1: Find data


```{r load-data,include=FALSE}
library('mlr')
library('ggplot2')
library('tidyr')
library('lime')
devtools::install_github("christophM/iml", ref = "feature-effects")
library('iml')
source('../code/prepare-wine-data.R')
```

```{r show-dist}
ggplot(wine) + 
  geom_bar(aes(x = quality)) + 
  scale_x_continuous("Wine Quality", labels = 1:10, breaks = 1:10)
```

# Step 2: Throw ML on your data

##

10x CV
Test linear regression model, decision tree, random forest


## {.center data-background=../images/comp-dog.gif data-background-size=contain}


## Step 2: Throw ML on your data

```{r benchmark}
library("mlr")
lrn.ranger = makeLearner("regr.ranger")
lrn.lm = makeLearner("regr.lm")
lrn.rpart = makeLearner("regr.rpart")

rdesc = cv5

lrns = list(lrn.ranger, lrn.lm, lrn.rpart)

tsk = makeRegrTask(data = wine, target = "quality")

bmr = benchmark(lrns, tsk, rdesc, measures = list(mae))
bmr_tab = getBMRAggrPerformances(bmr, as.df = TRUE)
knitr::kable(bmr_tab[-1])
```

=> The random forest (ranger) is the best model.



```{r final-model}
mod = train(lrn.ranger, tsk)

set.seed(42)
sample_size = 500
wine_subset = wine[sample(1:nrow(wine), size = sample_size),]

pred = Predictor$new(mod, data = wine_subset, y = "quality")
```


## Step 2: Throw ML on your data

```{r check-model}

preds = pred$predict(wine)
preds$actual = wine$quality
ggplot(preds, aes(x = actual, y = .prediction, group = actual)) + 
  geom_violin(aes(fill = ..n..)) +
  scale_x_continuous("Actual quality (jittered)", 
    labels = 1:10, breaks = 1:10) + 
  scale_y_continuous("Predicted quality", labels = 1:10, breaks = 1:10) + 
  scale_fill_gradient(low = "white", high = "darkblue", guide = "none")
```


## {data-background=../images/delivery.gif data-background-size=contain}


# Step 3: Profit {.center}

## {data-background=../images/done-here.gif data-background-size=contain}
```{r, echo=FALSE, out.width='80%', include = FALSE}
knitr::include_graphics("../images/done-here.gif" )
```

##  {.emphasizedabit}

Client: "We would love to learn some insights."



##  {.emphasizedabit .center data-background=../images/black-box.gif data-background-size=cover}

<div class="white">
Looking inside the black box
</div>

## What are the most important features?

```{r feature-importance}
imp = FeatureImp$new(pred, loss = "mae")
plot(imp)
```

## How do features affect predictions?

```{r feature-effects}
effs = FeatureEffects$new(pred)
plot(effs, ncols = 4)
```

## How do features affect predictions?

```{r feature-effect-alcohol}
eff = FeatureEffect$new(pred, "alcohol")
plot(eff)
```

## Rule of thumb what makes wine good?

```{r surrogate}
sbrl()
tree = TreeSurrogate$new(pred, maxdepth  = 2)
plot(tree)
```


## Customers want to have individual explanations

TODO: Image where customers checks wine?


## Exceptionally bad wine

```{r bad-wine}
# find bad wine in data
predictions = pred$predict(wine)

min_pred = min(predictions)
worst = wine[which(predictions == min_pred),]

# explain with shapley
knitr::kable(t(worst))
# explain with cf?

min_pred

```

## Shapley Value

TODO: Draw features as players and within coalitions

## Shapley Value

```{r shapley}
shap = Shapley$new(pred, x.interest = worst)
plot(shap)
```

## Volatile Acidity

```{r volatile-acidity}
eff = FeatureEffect$new(pred, "volatile.acidity", method = "pdp+ice")
plot(eff)
```
## Volatile Acidity

‘Although the presence of high amounts of VA is considered undesirable, in some cases a touch of volatility is no bad thing,’ said Natasha Hughes MW in her guide to wine ‘flaws’.


```{r volatile-acidity}

# Get wines with low volatile acidity
predictions = pred$predict(wine_subset)

wine2 = wine_subset
wine2$volatile.acidity = wine2$volatile.acidity + 0.2
predictions2 = pred$predict(wine2)

plot(hist(predictions2$.prediction - predictions$.prediction))

wine_improve = which(predictions2$.prediction - predictions$.prediction > 0.3)

eff = FeatureEffect$new(pred, "volatile.acidity", method = "ice")

res = eff$results
res$improves = res$.id %in% wine_improve

ggplot(res) + 
  geom_line(aes(x = volatile.acidity, y = .y.hat, group = .id), alpha = 0.1) + 
  facet_wrap("improves")
```

## LIME

TODO: Explain with LIME




## Sulfur

At low levels reduction causes truffle, mushroom, radish, and green olive-like smells to wine. These have been accepted as positive traits and increase complexity in the wine. At higher levels, reduction is turns into a fault, giving wines a pungent smell of rotten egg, onion, garlic, cooked cabbage, canned corn, or even burnt rubber. 

```{r sulfur}
```




