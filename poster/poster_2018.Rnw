\documentclass[portrait, a0paper]{tikzposter}
\usepackage{url}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{pdfpages}
\usepackage{xcolor}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
%\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{wrapfig}
\usepackage{xargs}
\usepackage{subfig}

\usepackage{hyperref}
\usetheme{Board}
\usecolorstyle[colorPalette=PurpleGrayBlue]{Germany}

\usebackgroundstyle{Default}
\title{Interpretable Machine Learning}
\author{Christoph Molnar  \url{christoph.molnar@stat.uni-muenchen.de}}
\institute{LMU Munich / Department of Statistics / Prof. Dr. Bernd Bischl}
%\titlegraphic{\includegraphics[scale=0.1]{figure/logo.png}}
\usetitlestyle{Empty}
% \colorlet{titlefgcolor}{black}
% \colorlet{titlebgcolor}{colorOne}

<<setup-child, include = FALSE, echo=FALSE>>=
library(mlr)
library(ggplot2)
library(gridExtra)
library(data.table)
library(ggExtra)
library(knitr)

options(digits = 3, width = 65, str = strOptions(strict.width = "cut", vec.len = 3))

opts_chunk$set(
  echo        = FALSE,
  prompt      = FALSE,
  keep.source = TRUE,
  strip.white = TRUE,
  cache       = FALSE,
  tidy        = FALSE,
  concordance = TRUE,
  message     = FALSE,
  warning     = FALSE,
  size        = 'scriptsize',
  fig.height  = 5.8,
  fig.width   = 8,
  fig.pos     = "h!",
  small.mar   = TRUE,
  eps         = FALSE,
  crop        = TRUE,
  fig.align   = "center",
  out.width   = "0.27\\textwidth"
  # fig.path    = "knit-figure/prada1-"
)

theme_update(axis.line = element_line(colour = "black"),
  panel.grid.major = element_line(colour = "grey80"),
  panel.grid.minor = element_line(colour = "grey80"),
  panel.border = element_blank(),
  panel.background = element_rect(fill = "transparent"),
  plot.background = element_rect(fill = "transparent"))
@


\begin{document}
\maketitle

\begin{columns}

\column{0.33}
\block{Machine Learning}{
Machine learning is a set of methods that allow computers to learn from data to make and improve predictions (e.g. cancer, weekly sales, credit default).
<<ml>>=
knitr::include_graphics("figure/programing-ml.png")
@
}
\block{Black Box Problem}{
<<iml>>=
knitr::include_graphics("figure/iml.png")
@

Solution: \textbf{Interpretable Machine Learning} is a set of methods and models that make it possible to explain the behaviour and predictions of machine learning systems.\cite{molnar}
}


\block{Why Explain?}{
We need to explain machine learning models to ... \cite{adadi2018peeking}:
% \begin{itemize}
% \item \textbf{Explain to justify}: Machine learning models make errors, can be unfair (e.g racial bias.) and make unexpected predictions. In those cases, we require a model to justify predictions.
% \item \textbf{Explain to control}: Understand more about the systems vulnerabilities and conditions for failure. e.g. know for what type of data it performs worse. 
% \item \textbf{Explain to improve}: Insights about model behaviour helps to improve the model.
% \item \textbf{Explain to discover}: A black box model is optimized to make predictions, not to give insights.
% Interpretability methods enable to extract insights from black box models.
% \end{itemize}
<<explain-to>>=
knitr::include_graphics("figure/explain.png")
@
}

\block{Example: Predict Rent}{

As an application example we train a machine learning model (here a random forest), which predicts the rent of an apartment based on the size of the living space, the location ("good" or "bad") and whether cats are allowed.

<<create-data, echo = FALSE>>=
set.seed(42)
n = 500
df = data.frame(location = sample(c("Good", "Bad"), prob = c(0.4, 0.6), size = n, replace = TRUE))
df$cats = factor(sample(c("Yes", "No"), size = n, replace = TRUE))
df$size = round(runif(n, min = 20, max = 140) )
df$rent = (12 * (df$location == "Bad") + 17 * (df$location == "Good")) * df$size + 100 * (df$cats == "Yes") + 50 * (df$cats == "Yes" & df$location == "Good") + rnorm(n, sd = 10)
@

<<rent>>=
knitr::include_graphics("figure/rent-ml.png")
@

<<train-model, echo=FALSE>>=
library(randomForest)
library(mlr)
library(iml)

tsk = makeRegrTask(data = df, target = "rent")
lrn = makeLearner("regr.randomForest")
rf = train(lrn, tsk)
pred = Predictor$new(rf, data = df, y = "rent")
@

}



\column{0.33}

\block{Feature Effect}{

The feature effect describes how changing a feature changes on average the prediction of all data points. \cite{friedman2001greedy}

<<ice, echo=FALSE, fig.width=12, fig.height=6>>=
eff = FeatureEffect$new(pred, "size", method = "pdp")
eff.res = eff$results
ggplot(eff.res)  + geom_line(aes(x=size, y = .y.hat), size = 3) +
  scale_x_continuous("Size of living area") + 
  scale_y_continuous("Predicted rent") +
  theme(aspect.ratio=2/4, text = element_text(size=35),
    panel.grid.major = element_blank(), panel.grid.minor = element_blank())
@

\textit{If we vary the size of the living area, we observe how the predicted rente increases with increasing size.}


%Caveat: ICE and PDP have trouble when the inputs are strongly correlated.
%In this case, use Accumulated Local Effects instead \cite{apley2016visualizing}.
}
\block{Feature Importance}{
Feature importance \cite{Fisher2018} tells us how much model error increases when we shuffle its values in the data (= "destroying" the relationship between the feature and the outcome).
The greater the increase in error, the more important the feature is.
<<feature-importance, echo=FALSE, fig.width=12, fig.height=8>>=
imp = FeatureImp$new(pred, "mae")

impres = imp$results

ggplot(impres) + geom_col(aes(y = importance, x = feature)) + coord_flip() +
  theme(aspect.ratio=2/4, text = element_text(size=30),
    panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  scale_x_discrete("") +
  scale_y_continuous("Feature Importance")
@
\textit{In the rent example the most important feature was the size of the living area. Shuffling the size feature increases the model error by a factor of 2.5.}
}
<<apartment, echo=FALSE,include=FALSE>>=
# Choose apartment
apartment = data.frame(location = factor("Good", levels = c("Bad", "Good")),
    cats = factor("No", levels = c("No", "Yes")),
  size = 50)
pred.rent = round(pred$predict(apartment)$.prediction, 0)
@
\block{Individual Predictions}{
Sometimes we want to explain why a certain prediction was made by a machine learning model.
\textit{Suppose we want to explain the 989Euro rent prediction for a $50m^2$ apartment in a good location and where cats are forbidden.}
<<rent2, out.width="0.2\\textwidth">>=
knitr::include_graphics("figure/rent.png")
@

<<counterfact, echo=FALSE, include = FALSE>>=
apartment.mod = apartment
apartment.mod$cats = factor("Yes", levels = c("No", "Yes"))
pred.new = round(pred$predict(apartment.mod[,1:3, drop=FALSE]), 0)
@

\textbf{Counterfactual Explanation} \\
A counterfactual explanation of a prediction describes the smallest changes to the feature values (inputs) that change the prediction to a predefined output. \\
\textit{How do the inputs for the 989 Euro apartment have to change so that the predicted rent is over 1100 Euros?
Answer (one of many possible ones): If cats were allowed, the predicted rent would be \Sexpr{pred.new} Euros.}
\\ \\
\textbf{Shapley Values} \\
From a game theory point of view, the feature values are players in a cooperative game who receive the value of the prediction as a payout.
The Shapley Values method \cite{vstrumbelj2014explaining} splits the difference between the prediction and the average prediction fairly among the feature values.

<<shapley, echo=FALSE, fig.width=12, fig.height=6>>=
shap = Shapley$new(pred)
shap$explain(apartment)
avg.pred  = round(mean(pred$predict(df)[[1]]))
plot(shap) +
  theme(aspect.ratio=2/4, text = element_text(size=30),
    panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  scale_y_continuous("Contribution to difference of \nprediction minus avg. prediction") +
  scale_x_discrete("")

@
\textit{The predicted value of the apartment is 989Euros, the average prediction of all apartments is \Sexpr{avg.pred}. The negative difference is explained by its small size and the ban on cats. The good location has a positive effect on the predicted rent.}

}

\column{0.33}

\block{Interpretable Models}{
Intrinsically interpretable models learn simple relationships between input and output.
\\ \\
\textbf{Linear Regression Models}\\
%The linear regression model models the outcome y as a weighted sum of the input features.

%\[y = \beta_0 + \beta_1 \cdot \text{Feature}_1 + \ldots + \beta_p \cdot \text{Feature}_p\]

%The goal is to find the best possible weights $\beta$.
<<lm>>=
knitr::include_graphics("figure/lm.png")
@
\textit{In the rent example, the following weights were learned:}
<<train-model-lm, echo=FALSE, include=FALSE>>=
mod.lm = lm(rent ~ ., data = df)
cf = round(coef(mod.lm))
knitr::kable(cf, format = 'latex')
@

$\text{rent} = \Sexpr{cf["(Intercept)"]} + \Sexpr{cf["size"]} \cdot \text{size}  + \Sexpr{cf["locationGood"]} \cdot I_{\text{good location}} +  \Sexpr{cf["catsYes"]} \cdot I_{\text{cats allowed}}$
\textit{The predicted rent increases by \Sexpr{cf["size"]} Euros per each $m^2$.}
%Changing the location from "Bad" to the "Good" increases the predicted rent by \Sexpr{coef(mod.lm)["locationGood"]}.
%Linear models have many extensions, like Generalized Linear Models (GLMs) or Generalized Additive Models (GAMs) that allow more flexiblity.
\\ \\
\textbf{Decision Trees}\\
Decision trees divide the data into smaller subsets based on 'decisions' made on input features. 

<<train-model-tree, echo=FALSE, out.width="0.28\\textwidth">>=
library(partykit)
mod.tree = ctree(rent ~ ., data = df, control = ctree_control(maxdepth = 2))
#plot(mod.tree, inner_panel = node_inner(mod.tree, pval = FALSE), type='simple')
knitr::include_graphics("figure/tree.png")
@
\textit{The tree predicts a rent of 1908 Euros for an apartment larger than 77 $m^2$ in a good location.}
\\ \\
\textbf{Decision Rules} \\
Decision rules are IF-THEN statements that consist of a condition and a prediction.
One or more rules can be used to make predictions. For example:
<<train-model-rules, echo=FALSE, includee=FALSE, eval=FALSE>>=

library("RWeka")
library(rJava)

extract.rules.jrip = function (rule) {
rules = scan(text=.jcall(rule$classifier, "S", "toString"), sep="\n", what="")
# removes text
rules = rules[-c(1, 2, length(rules))]
rules = gsub("\\([0-9]*\\.[0-9]\\/[0-9]*\\.[0-9]\\)", "", rules)
rules = as.matrix(rules)[-c(1:2, 6), ,drop=FALSE]
rules  = data.frame(rules)
if (nrow(rules) == 0) {
return(NULL)
} else {
knitr::kable(rules, format = 'latex')
}
}

df2 = df
df2$rent = cut(df$rent, breaks = quantile(df$rent, probs = seq(from = 0, to = 1, length.out = 20)))
df2$size = cut(df$size, breaks = quantile(df$size, probs = seq(from = 0, to = 1, length.out = 5)))
rule = JRip(rent ~ ., data = df2)
extract.rules.jrip(rule)
@
\textit{
IF $90m^2\leq \text{size} < 110m^2$  AND location $=$ Good THEN the rent is between 1540 and 1890 EUR}

}
\block{Surrogate Models}{
\textbf{Global Surrogate Models} \\
%Flexible but less interpretable models often predict the data better than interpretable models.
Intrinsically interpretable models (e.g. a tree) can be used to increase interpretability of a black box model (e.g. a neural network) by approximating its predictions by acting as a surrogate model that can be interpreted. 
<<surrogate>>=
knitr::include_graphics("figure/global-surrogate.png")
@

\textbf{Local Surrogate Models}\cite{ribeiro2016should} \\
To explain an individual prediction, replace a  complex model with a locally weighted, interpretable model (e.g. decision tree).
Data points that are close to the data point of interest get a high weight.

<<lime, echo=FALSE>>=
# locmod = LocalModel$new(pred, k = 2)
# locmod$explain(df[1,1:3])
# plot(locmod)
knitr::include_graphics("figure/local-surrogate.png")
@
}

\block{References}{
\begingroup
\renewcommand{\section}[2]{}%
\small
\bibliographystyle{plain}
\bibliography{Bib}
\endgroup
}


\end{columns}
\end{document}
