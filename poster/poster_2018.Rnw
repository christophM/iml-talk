\documentclass[portrait, a0paper]{tikzposter}
\usepackage{url}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{pdfpages}
\usepackage{xcolor}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
%\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{wrapfig}
\usepackage{xargs}
\usepackage{subfig}

\definecolorpalette{GrayOrangeBlue}{
\definecolor{colorOne}{HTML}{C0C0C0}
\definecolor{colorTwo}{HTML}{CCCCCC}
\definecolor{colorThree}{HTML}{009440}
}
\usepackage{hyperref}
\usetheme{Desert}
\usecolorstyle[colorPalette=BlueGrayOrange]{Russia}
\usebackgroundstyle{Rays}
\title{Interpretable Machine Learning}
\author{Christoph Molnar \\ }
\institute{
LMU Munich / Department of Statistics/ \url{christoph.molnar@stat.uni-muenchen.de} \\
Prof. Dr. Bernd Bischl}
%\titlegraphic{\hspace{-3200px}\includegraphics[scale=0.3]{figures/logo.png}}
\usetitlestyle{Empty}
\colorlet{titlefgcolor}{black}
\colorlet{titlebgcolor}{colorOne}

<<setup-child, include = FALSE, echo=FALSE>>=
library(mlr)
library(ggplot2)
library(gridExtra)
library(data.table)
library(ggExtra)
library(knitr)

options(digits = 3, width = 65, str = strOptions(strict.width = "cut", vec.len = 3))

opts_chunk$set(
  echo        = FALSE,
  prompt      = FALSE,
  keep.source = TRUE,
  strip.white = TRUE,
  cache       = FALSE,
  tidy        = FALSE,
  concordance = TRUE,
  message     = FALSE,
  warning     = FALSE,
  size        = 'scriptsize',
  fig.height  = 5.8,
  fig.width   = 8,
  fig.pos     = "h!",
  small.mar   = TRUE,
  eps         = FALSE,
  crop        = TRUE,
  fig.align   = "center",
  out.width   = "0.3\\textwidth"
  # fig.path    = "knit-figure/prada1-"
)

theme_update(axis.line = element_line(colour = "black"),
  panel.grid.major = element_line(colour = "grey80"),
  panel.grid.minor = element_line(colour = "grey80"),
  panel.border = element_blank(),
  panel.background = element_rect(fill = "transparent"),
  plot.background = element_rect(fill = "transparent"))
@


\makeatletter
\renewcommand\section{%
\@startsection
{section}% name
{1}% level
{\z@}% indent
{0ex \@plus -1ex \@minus -.2ex}% beforeskip, changed!
{2.3ex \@plus.2ex}% afterskip
{\normalfont\Large\bfseries}% style
}
\makeatother
\begin{document}
\maketitle

\begin{columns}

\column{0.33}
\block{Machine learning}{
Machine learning is a set of techniques that lets a computer learn from data to make predictions by finding patterns in the data, in contrast to of telling the computer explicitly how to make predictions.
Machine learning can be used to predict credit default, risk for cancer, number of sales in the next week and much more.
The big disadnvantage of the machine learning approach is that the program now is a black box for us. 

\includegraphics[width=0.25\textwidth]{figure/black-box.png}

This poster presents methods for interpretability that are model-agnostic (i.e. can be applied to any black box model), which help to explain the behaviour of machine learning models and individual predictions.
It also presents models that are considered intrinsically interpretable.

}

\block{Example: Predict Rent}{
To illustrate the intepretability methods, we use a machine learning model that predicts the rent of an appartment given the size of the living area, the location and if cats are allowed.
The data is simulated and any other data would have worked as well.
For this example I used random forests, but other models, like a neural network, a linear regression model or boosting would have been possible as well, because the interpretability methods are all model-agnostic.

<<create-data, echo = FALSE>>=
set.seed(42)
n = 500
df = data.frame(location = sample(c("Inner city", "Outer city"), prob = c(0.4, 0.6), size = n, replace = TRUE))
df$cat.allowed = factor(sample(c("Yes", "No"), size = n, replace = TRUE))
df$size = round(runif(n, min = 20, max = 100) + (df$location != "Inner city") * abs(round(rnorm(n, sd = 20), 1)), 0)
df$rent = (12 * (df$location == "Outer city") + 17 * (df$location == "Inner city")) * df$size + 100 * (df$cat.allowed == "Yes") + 50 * (df$cat.allowed == "Yes" & df$location == "Inner city")
@

<<train-model, echo=FALSE>>=
library(randomForest)
library(mlr)
library(iml)

tsk = makeRegrTask(data = df, target = "rent")
lrn = makeLearner("regr.randomForest")
rf = train(lrn, tsk)
pred = Predictor$new(rf, data = df, y = "rent")
@

}

\block{Why explain?}{
We need to explain machine learning models to\cite{adadi2018peeking}:
TODO: COPY FROM SLIDES
\begin{itemize}
\item \textbf{Explain to justify}: Machine learning models make errors, can be unfair (e.g racial bias.) and make unexpected predictions. In those cases, we require a model to justify predictions.
\item \textbf{Explain to control}: Understand more about the systems vulnerabilities and conditions for failure. e.g. know for what type of data it performs worse. 
\item \textbf{Explain to improve}: Insights about model behaviour helps to improve the model.
\item \textbf{Explain to discover}: A black box model is optimized to make predictions, not to give insights.
Interpretability methods enable to extract insights from black box models.
\end{itemize}
}



\block{Feature importance}{
We can measure importance of the inputs as the drop in performance of the model if we would not know the features.
Not knowing the feature can be simulated by shuffling the feature in the data: We break the assocation between the feature and the target.
If the performance of the model drops a lot, the features was important.
<<feature-importance, echo=FALSE>>=
imp = FeatureImp$new(pred, "mae")
plot(imp)
@
In our little rent example, the most important feature was the size of the living area. 

Caveats: Doesn't work well when the features are highly correlated.
}

\column{0.33}

\block{Feature effects}{
How does, on average, a feature input affect the prediction?
The effect of a feature on the prediction can become complex, depending on the underlying model used.

The simplest way to analyze this for an individual prediction is called Individual Conditional Expectations: 
For an individual data point, we vary one of the features and observe how the prediction changes.
If we do this for all data points and plot this, we get the individual conditional expectation plot \cite{goldstein2015peeking}.
When we average those lines (black), the we get the partial dependence plot (yellow line) \cite{friedman2001greedy}:

We know the prediction for this point, and we can visualize how the prediction changes when we vary the input.
For example we can vary the size of the living area and observe how the prediction of the model changes. 

<<ice, echo=FALSE>>=
eff = FeatureEffect$new(pred, "size", method = "pdp+ice")
plot(eff, rug = FALSE)  + ggtitle("Effect of size on predicted rent") +
  scale_x_continuous("Size of appartment") + 
  scale_y_continuous("Predicted rent")
@

Caveat: ICE and PDP have trouble when the inputs are strongly correlated.
In this case, use Accumulated Local Effects instead \cite{apley2016visualizing}.
}

\block{Individual prediction explanation}{

<<appartment, echo=FALSE,include=FALSE>>=
# Choose appartment
appartment = df[10,]
pred.rent = round(pred$predict(appartment)$.prediction, 0)
@

We have a couple of options to explain why a particular prediction was made by a machine learning model.
Suppose the rent model predicts \Sexpr{pred.rent} Euros rent, for an appartment in the inner city with a living area of 85 $m^2$ and where cats are allowed. 
How can we explain this prediction?


<<counterfact, echo=FALSE, include = FALSE>>=
appartment.mod = appartment
appartment.mod$cat.allowed = factor("No", levels = c("No", "Yes"))
pred.new = round(pred$predict(appartment.mod[,1:3, drop=FALSE]), 0)
@
\textbf{Counterfactual explanations}: 
A counterfactual explanation of a prediction describes the smallest change to the feature values (inputs) that changes the prediction to a predefined output.
For example: How does the input have to change so that the prediction changes below 1200 Euros?
Answer: If cats were not allowed, the predicted rent would be \Sexpr{pred.new} Euros.


\textbf{Shapley Values}: 
A prediction can be seen as a cooperative game. 
The feature values (inputs) are players in a game and cooperate to achieve a game payout, which is the prediction.
We want to fairly distribute the difference of the prediction to the average prediction among the features.
Game theory has developed a method called Shapley values that can fairly do this type of distribution.
For machine learning, it works by repeatedly simulating that only a subset of feature values is known and computing how the prediction would look like.

<<shapley, echo=FALSE>>=
shap = Shapley$new(pred)
shap$explain(appartment[,1:3])
plot(shap)
@

\textbf{Local surrogate models}: 
The idea behind local surrogate models is to replace the complex model with an interpretable model (e.g. regression), but only locally. 
This means we fit an individual interpretable model for the data point for which we want to explain the prediction.
The closer data points that are to the data point of interest the higher the weight in the model calculation.
Interpretation depends on the chosen model, see poster box on the top right.

<<lime, echo=FALSE>>=
# locmod = LocalModel$new(pred, k = 2)
# locmod$explain(df[1,1:3])
# plot(locmod)
@

}

\column{0.33}

\block{Interpretable Models}{
Some models are intrinsically interpretable, because they learn simple relationships between input and output like a decision tree or a weighted sum of the inputs.
The big disadvantage of intrinsically interpretable models is that they often predict the data less well.

\section*{Linear Regression Models}
The linear regression model models the outcome as a weighted sum:

\[y = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p\]

$x_1$ to $x_p$ are the p features that are used to predict the outcome y. 
The goal is to find the best possible weights, which can be found for example via the least squares algorithm.

In our rent example, the folowing weights are found:
<<train-model-lm, echo=FALSE, include=FALSE>>=
mod.lm = lm(rent ~ ., data = df)
cf = round(coef(mod.lm))
knitr::kable(cf, format = 'latex')
@

$$rent = \Sexpr{cf["(Intercept)"]} + \Sexpr{cf["size"]} \cdot size  \Sexpr{cf["locationOuter city"]} \cdot I_{\text{outer city}} +  \Sexpr{cf["cat.allowedYes"]} \cdot I_{\text{cats allowed}}$$


A positive weight means that the feature increases the prediction, negative decreases the prediction.
An increase of the living area increases the predicted rent by \Sexpr{coef(mod.lm)["size"]}.
Changing the location from the industrial complex (the reference category) to the inner city increases the predicted rent by \Sexpr{coef(mod.lm)["locationInner city"]}.
Linear models have many extensions, like Generalized Linear Models (GLMs) or Generalized Additive Models (GAMs) that allow more flexiblity.

\section*{Decision Trees}
Decision trees are like the decision trees we know from other areas, except that they are learned based on data in machine learning.
Decision trees partition the data into smaller subsets, based on 'decisions' made on the input features.

Let's train a decision tree on the rent data:
<<train-model-tree, echo=FALSE>>=
library(partykit)
mod.tree = ctree(rent ~ ., data = df, control = ctree_control(maxdepth = 2))
plot(mod.tree, inner_panel = node_inner(mod.tree, pval = FALSE), type='simple')
@
The living area is selected as the first split feature. 



\section*{Decision rules}
A decision rule is a simple IF-THEN statement consisting of a condition and a prediction.
For example: IF it rains today AND if itâ€™s April (condition), THEN it will rain tomorrow (prediction).
A single decision rule or a combination of several rules can be used to make predictions.

<<train-model-rules, echo=FALSE, includee=FALSE, eval=FALSE>>=

library("RWeka")
library(rJava)

extract.rules.jrip = function (rule) {
rules = scan(text=.jcall(rule$classifier, "S", "toString"), sep="\n", what="")
# removes text
rules = rules[-c(1, 2, length(rules))]
rules = gsub("\\([0-9]*\\.[0-9]\\/[0-9]*\\.[0-9]\\)", "", rules)
rules = as.matrix(rules)[-c(1:2, 6), ,drop=FALSE]
rules  = data.frame(rules)
if (nrow(rules) == 0) {
return(NULL)
} else {
knitr::kable(rules, format = 'latex')
}
}

df2 = df
df2$rent = cut(df$rent, breaks = quantile(df$rent, probs = seq(from = 0, to = 1, length.out = 20)))
df2$size = cut(df$size, breaks = quantile(df$size, probs = seq(from = 0, to = 1, length.out = 5)))
rule = JRip(rent ~ ., data = df2)
extract.rules.jrip(rule)
@
A rule could be: 

IF living area $\in 90 \text{ and } 110 m^2$  AND location $=$ inner city THEN the rent is between 1540 and 1890 EUR

Other approaches: 
RuleFit CITE, SLIM, \ldots

\section*{Surrogate Models}
Often these intrinsically interpretable models are much worse than more flexible models, but they can still be used to increase interpretabilit by approximating a complex model with an interpretable model. Predict the predictions of the complex model with an intrinsically interpretable model and interpret the latter. For example, interpret a neural network by approximating its predictions with a decision tree. 


}


\block{References}{
\begingroup
\renewcommand{\section}[2]{}%
\small
\bibliographystyle{plain}
\bibliography{Bib}
\endgroup
}
\end{columns}
\end{document}
