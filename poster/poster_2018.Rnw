\documentclass[portrait, a0paper]{tikzposter}
\usepackage{url}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{pdfpages}
\usepackage{xcolor}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
%\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{wrapfig}
\usepackage{xargs}
\usepackage{subfig}

\definecolorpalette{GrayOrangeBlue}{
\definecolor{colorOne}{HTML}{C0C0C0}
\definecolor{colorTwo}{HTML}{CCCCCC}
\definecolor{colorThree}{HTML}{009440}
}
\usepackage{hyperref}
%\usetheme{Desert}
%\usecolorstyle[colorPalette=BlueGrayOrange]{Russia}
\usebackgroundstyle{Rays}
\title{Interpretable Machine Learning}
\author{Christoph Molnar \\ }
\institute{
LMU Munich / Department of Statistics/ \url{christoph.molnar@stat.uni-muenchen.de} \\
Prof. Dr. Bernd Bischl}
%\titlegraphic{\hspace{-3200px}\includegraphics[scale=0.3]{figures/logo.png}}
\usetitlestyle{Empty}
\colorlet{titlefgcolor}{black}
\colorlet{titlebgcolor}{colorOne}

<<setup-child, include = FALSE, echo=FALSE>>=
library(mlr)
library(ggplot2)
library(gridExtra)
library(data.table)
library(ggExtra)
library(knitr)

options(digits = 3, width = 65, str = strOptions(strict.width = "cut", vec.len = 3))

opts_chunk$set(
  echo        = FALSE,
  prompt      = FALSE,
  keep.source = TRUE,
  strip.white = TRUE,
  cache       = FALSE,
  tidy        = FALSE,
  concordance = TRUE,
  message     = FALSE,
  warning     = FALSE,
  size        = 'scriptsize',
  fig.height  = 5.8,
  fig.width   = 8,
  fig.pos     = "h!",
  small.mar   = TRUE,
  eps         = FALSE,
  crop        = TRUE,
  fig.align   = "center",
  out.width   = "0.27\\textwidth"
  # fig.path    = "knit-figure/prada1-"
)

theme_update(axis.line = element_line(colour = "black"),
  panel.grid.major = element_line(colour = "grey80"),
  panel.grid.minor = element_line(colour = "grey80"),
  panel.border = element_blank(),
  panel.background = element_rect(fill = "transparent"),
  plot.background = element_rect(fill = "transparent"))
@


\makeatletter
\renewcommand\section{%
\@startsection
{section}% name
{1}% level
{\z@}% indent
{0ex \@plus -1ex \@minus -.2ex}% beforeskip, changed!
{2.3ex \@plus.2ex}% afterskip
{\normalfont\Large\bfseries}% style
}
\makeatother
\begin{document}
\maketitle

\begin{columns}

\column{0.33}
\block{Machine learning}{
Machine learning is a set of methods that let a computer learn from data to make predictions by finding patterns.
Can be used to predict credit default, cancer, weekly sales ...
<<ml>>=
knitr::include_graphics("figure/ml.png")
@
}
\block{Black Box Problem}{
The big disadvantage of the machine learning approach is that the  learned program is intransparent to us. 

\includegraphics[width=0.25\textwidth]{figure/teaser.png}

Interpretable machine learning is a set of methods and models that enable to explain the behaviour and predictions of machine learning systems.\cite{molnar}
}


\block{Why explain?}{
We need to explain machine learning models to \cite{adadi2018peeking}:
\begin{itemize}
\item \textbf{Explain to justify}: Machine learning models make errors, can be unfair (e.g racial bias.) and make unexpected predictions. In those cases, we require a model to justify predictions.
\item \textbf{Explain to control}: Understand more about the systems vulnerabilities and conditions for failure. e.g. know for what type of data it performs worse. 
\item \textbf{Explain to improve}: Insights about model behaviour helps to improve the model.
\item \textbf{Explain to discover}: A black box model is optimized to make predictions, not to give insights.
Interpretability methods enable to extract insights from black box models.
\end{itemize}
}

\block{Example: Predict Rent}{
As example we use a machine learning model that predicts the rent of an appartment given the size of its living area, whether the location is good or bad and if cats are allowed.

<<create-data, echo = FALSE>>=
set.seed(42)
n = 500
df = data.frame(location = sample(c("Good", "Bad"), prob = c(0.4, 0.6), size = n, replace = TRUE))
df$cats = factor(sample(c("Yes", "No"), size = n, replace = TRUE))
df$size = round(runif(n, min = 20, max = 140) )
df$rent = (12 * (df$location == "Bad") + 17 * (df$location == "Good")) * df$size + 100 * (df$cats == "Yes") + 50 * (df$cats == "Yes" & df$location == "Good") + rnorm(n, sd = 10)
@

<<rent>>=
knitr::include_graphics("figure/rent.png")
@

<<train-model, echo=FALSE>>=
library(randomForest)
library(mlr)
library(iml)

tsk = makeRegrTask(data = df, target = "rent")
lrn = makeLearner("regr.randomForest")
rf = train(lrn, tsk)
pred = Predictor$new(rf, data = df, y = "rent")
@

}



\column{0.33}

\block{Feature effects}{
The feature effects describe how changing one of the features (on average) changes the prediction.
The effects can be visualized for individual data points (called Individual Conditional Expectations \cite{goldstein2015peeking}) or as an averaged curve (called Partial Dependence Plots \cite{friedman2001greedy}).

<<ice, echo=FALSE, fig.width=12, fig.height=6>>=
eff = FeatureEffect$new(pred, "size", method = "ale")
eff.res = eff$results
ggplot(eff.res)  + geom_line(aes(x=size, y = .ale), size = 3) +
  scale_x_continuous("Feature Effect") + 
  scale_y_continuous("Predicted rent") +
  theme(aspect.ratio=2/4, text = element_text(size=35),
    panel.grid.major = element_blank(), panel.grid.minor = element_blank())
@
For example we can vary the size of the living area and observe how the prediction of the model changes. 


%Caveat: ICE and PDP have trouble when the inputs are strongly correlated.
%In this case, use Accumulated Local Effects instead \cite{apley2016visualizing}.
}
\block{Feature importance}{
The feature importance tells us how much the model accuracy drops when we don't know a feature, which is simulated by shuffling the feature values.
The bigger the drop, the more important.
<<feature-importance, echo=FALSE, fig.width=12, fig.height=8>>=
imp = FeatureImp$new(pred, "mae")

impres = imp$results

ggplot(impres) + geom_col(aes(y = importance, x = feature)) + coord_flip() +
  theme(aspect.ratio=2/4, text = element_text(size=30),
    panel.grid.major = element_blank(), panel.grid.minor = element_blank())
@
In the rent example the most important feature was the size of the living area. 
}
\block{Individual predictions}{

<<appartment, echo=FALSE,include=FALSE>>=
# Choose appartment
appartment = data.frame(location = factor("Good", levels = c("Bad", "Good")),
    cats = factor("Yes", levels = c("No", "Yes")),
  size = 50)
pred.rent = round(pred$predict(appartment)$.prediction, 0)
@

We have a couple of options to explain why a particular prediction was made by a machine learning model.
Suppose we want to explain the prediction of the following appartment. 
<<rent2>>=
knitr::include_graphics("figure/rent.png")
@

<<counterfact, echo=FALSE, include = FALSE>>=
appartment.mod = appartment
appartment.mod$cats = factor("No", levels = c("No", "Yes"))
pred.new = round(pred$predict(appartment.mod[,1:3, drop=FALSE]), 0)
@
\section*{Counterfactual explanations}
A counterfactual explanation of a prediction describes the smallest change to the feature values (inputs) that changes the prediction to a predefined output.
For example: How does the inputs have to change so that the prediction changes below 900 Euros?
Answer (one of many possible ones): If cats were allowed, the predicted rent would be \Sexpr{pred.new} Euros.


\section*{Shapley Values}
From a game theoretical perspective, the feature values are like players in a game and cooperate to achieve a game payout, which is the prediction.
A method called Shapley Values TODO:CITE fairly attributes the difference between the prediction and the average prediction among the feature values to explain a prediction.

<<shapley, echo=FALSE, fig.width=12, fig.height=6>>=
shap = Shapley$new(pred)
shap$explain(appartment)
plot(shap) +
  theme(aspect.ratio=2/4, text = element_text(size=30),
    panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  scale_y_continuous("Contribution to prediction") +
  scale_x_discrete("")

@



}

\column{0.33}

\block{Interpretable Models}{
Intrinsically interpretable models learn simple relationships between input and output,
but they often predict the data less well.

\section*{Linear Regression Models}
The linear regression model models the outcome y as a weighted sum of the input features.

\[y = \beta_0 + \beta_1 \cdot \text{Feature}_1 + \ldots + \beta_p \cdot \text{Feature}_p\]

The goal is to find the best possible weights $\beta$.
In the rent example, the following model is learned.
<<train-model-lm, echo=FALSE, include=FALSE>>=
mod.lm = lm(rent ~ ., data = df)
cf = round(coef(mod.lm))
knitr::kable(cf, format = 'latex')
@

$$\text{rent} = \Sexpr{cf["(Intercept)"]} + \Sexpr{cf["size"]} \cdot \text{size}  + \Sexpr{cf["locationGood"]} \cdot I_{\text{good location}} +  \Sexpr{cf["catsYes"]} \cdot I_{\text{cats allowed}}$$


An increase of the living area increases the predicted rent by \Sexpr{coef(mod.lm)["size"]}.
Changing the location from "Bad" to the "Good" increases the predicted rent by \Sexpr{coef(mod.lm)["locationGood"]}.
%Linear models have many extensions, like Generalized Linear Models (GLMs) or Generalized Additive Models (GAMs) that allow more flexiblity.

\section*{Decision Trees}
Decision trees partition the data into smaller subsets, based on 'decisions' made on the input features. 
The prediction is based on the leaf node we end up in.

A decision tree trained on the rent data:
<<train-model-tree, echo=FALSE, out.width="0.28\\textwidth">>=
library(partykit)
mod.tree = ctree(rent ~ ., data = df, control = ctree_control(maxdepth = 2))
#plot(mod.tree, inner_panel = node_inner(mod.tree, pval = FALSE), type='simple')
knitr::include_graphics("figure/tree.png")
@
For example, the tree predicts a rent of 1908 Euros for an appartment bigger than 77 $m^2$ in a good location.


\section*{Decision Rules}
A decision rule is a simple IF-THEN statement consisting of a condition and a prediction.
One or many rules can be used to make predictions.

<<train-model-rules, echo=FALSE, includee=FALSE, eval=FALSE>>=

library("RWeka")
library(rJava)

extract.rules.jrip = function (rule) {
rules = scan(text=.jcall(rule$classifier, "S", "toString"), sep="\n", what="")
# removes text
rules = rules[-c(1, 2, length(rules))]
rules = gsub("\\([0-9]*\\.[0-9]\\/[0-9]*\\.[0-9]\\)", "", rules)
rules = as.matrix(rules)[-c(1:2, 6), ,drop=FALSE]
rules  = data.frame(rules)
if (nrow(rules) == 0) {
return(NULL)
} else {
knitr::kable(rules, format = 'latex')
}
}

df2 = df
df2$rent = cut(df$rent, breaks = quantile(df$rent, probs = seq(from = 0, to = 1, length.out = 20)))
df2$size = cut(df$size, breaks = quantile(df$size, probs = seq(from = 0, to = 1, length.out = 5)))
rule = JRip(rent ~ ., data = df2)
extract.rules.jrip(rule)
@

One decision rule learned for the rent prediction is:

IF living area $\in 90 \text{ and } 110 m^2$  AND location $=$ Good THEN the rent is between 1540 and 1890 EUR

\section*{Surrogate Models}
Intrinsically interpretable models can also be used as to increase interpretability of a black box model by approximating its predictions, acting as a surrogate model which can be interpreted. 
Example: Neural network as black box model, decision tree as surrogate model.


\section*{Local surrogate models}
The idea behind local surrogate models is to replace the complex model with an individual, locally weighted interpretable model (e.g. regression) for the data point for which we want to explain the prediction.

<<lime, echo=FALSE>>=
# locmod = LocalModel$new(pred, k = 2)
# locmod$explain(df[1,1:3])
# plot(locmod)
knitr::include_graphics("figure/local-surrogate.png")
@
}

\block{References}{
\begingroup
\renewcommand{\section}[2]{}%
\small
\bibliographystyle{plain}
\bibliography{Bib}
\endgroup
}


\end{columns}
\end{document}
