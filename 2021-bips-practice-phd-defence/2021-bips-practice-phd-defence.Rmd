---
title: "Model-agnostic Interpretable Machine Learning"
author: "Christoph Molnar"
date: "2021-08"
institute: "BIPS bremen"
output: ioslides_presentation
---



```{r, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, 
  warning = FALSE,
  message = FALSE, 
  fig.align='center', 
  out.width = '90%',
  dev = "CairoPNG")

ggplot2::theme_set(ggplot2::theme_bw(base_size = 15))
```

## Some ideas

- Cite pitfalls at many places in the talk
- INtroduction with History: INterweave motivation, and also cite history paper


## Outline

- A Brief History of Interpretable Modeling
- Model-agnostic interpretability
- Methods PDP and PFI
- What I did in my PhD 
    - Conditional Feature for PFI and PDP Importance Using Subgroups
    - Statistical Inference for PDP and PFI
    - Measuring Model Complexity

## Before You Pin Me Down

To get this out of the way, here is how I use "Interpretable Machine Learning":

It's an umbrella term for approaches that analyze or describe prediction mechanisms in models that were learned on data.

## A Brief History of Interpretable Modeling


* 1800s: Linear regression models
* 1960s: Rule-based ML
* 2001:  Random Forest
* 2012:  Deep Learning Boom kicked off
* 2016:  Interpretable ML / Explainable AI boom


Interpretable ML has 200 year old roots, but consolidation only started in 2016.

TODO: Add citation report and or google trends image

## Why Interpretability?

* The more non-linear and complex a model, the more difficult to understand how it operates
* Trust
* Debugging
* Control 
* ...

## Interpretable Models

* GAM and other linear models
* Trees and decision rules
* More complex models, but interpretation requires more work or post-processing

## Interpretable Models

```{r, echo = FALSE}
knitr::include_graphics("../images/white-box.png")
```


# Model-agnostic Interpretation

Basic idea: Treat the model as a black box, not allowed to look inside.

Idea very powerful: Allows modularity, works for most models, results are comparable across models 

TODO: CITE SIPA paper here

## Interpreting Models

```{r, echo=FALSE}
knitr::include_graphics("../images/black-box.png")
```

## Model-agnostic Methods

```{r, echo=FALSE}
knitr::include_graphics("../images/agnostic-black-box.png")
```



# Methodological Background

## Individual Conditional Expectations (ICE)

maybe use ice + pdp animation, but split into individual frames so they are clickable

## Partial Dependence Plot (PDP)

The theoretical construct:
$$ PD(x) = \mathbb{E}_{X_{C}}[\hat{f}(x, X_{C})] $$

The estimator:
$$\widehat{PD} = \frac{1}{n_2}\sum_{i=1}^{n_2} \hat{f}(x, x_{C}^{(i)})$$

What it looks like:

TODO: 1D-plot and 2D-plot


## Permutation Feature Importance PFI

The construct:
$$PFI =  \mathbb{E}_{Y \tilde{X}_S X_C}(L(Y, \hat{f}(\tilde{X}_S, X_C))) - \mathbb{E}_{Y X_S X_C}(L(Y,\hat{f}(X_S, X_C)))$$

The algorithm (simples form): CITE Breiman

- Measure original loss $L$ on test data.
- Permute features $X_S$ to get sample $\tilde{X}_S$.
- Get prediction with $\tilde{X}_S$, measure loss again.
- Measure difference between losses.
- Interpretation: PFI is performance drop when "destroying" information about feature.


## PFI and PDP similarity

CITE paper GC here




# Difference between model-agnostic and model-specific interpetation

* Analyzing components vs. analyzing behavior
* Components not very powerful, unless they directly describe behavior
* For complex model, components do not directly tell us about behavior of model
* Behavior more comparable


# Summary of Thesis

TODO: Draw image with Ipad

In my thesis I did a lot of consolidation work:

- Book
- R package
- History paper
- Pitfalls paper

What if we embrace black box view of models?

Special focus on old, established tools, and make them fit for right use in practice

Also some more specific papers

* subgroups
* inference
* quantifying

and some contributions to

* MOC
* relative feature importance
* SIPA
* feature importance and PDP (GC)

# Paper: Subgroups

## Problem: Dependent features

Show example with dependent features for PFI

## Solution: Conditional variants

Show solution

* But: Not meaningful for PDP, or other measure more useful (LOESS)
* But: Conditioning unclear
* But: Intepretation difficult

## Our solution: Permute in subgroups

TODO: Show algorithm

## Some results from our paper

TODO: Show results or application

# Paper: IML Inference

## Problem: Uncertainty quantification

PDP and PFI and most other just descriptive tools
Often practictioner draw confidence intervals
but only based on monte carlo approximation

* But what is even the ground truth?
* And what about the uncertainty in the model fit?
* How could we estimate the uncertainty?

## Solution: Treat as statistical estimator


## How to Compute the variance

## Problem of too low variance estimates

# Paper: Quantifying

TODO: Copy as much as possible from the talk that already exists for this paper

Motivation: want to quantify in a model-agnostic way how complex a model is

## Functional Decomposition

Explain how functional decomposition works

## ALE

Formula: 

TODO: For estimation show the images 

## The three measures

* Mean effect complexity
* non linearity
* number of features


## Using in benchmarking (multi-objective)  


# Inference of Feature Effects and Importance in Machine Learning

## Status-Quo

- For inference, practitioner rely on interpretable models
- remember: interpretable models have parameters that map to data-generating process
- but: situations where more flexible models match DGP better (measured in test loss)
- we have: model-agnostic interpretation
- But: only describes the model behavior
- Question: Can we link PD and PFI to the DGP? and even do inference?

## Example

TODO: Show PDP and PFI from some papers?

## Linking external tools to DGP

- Hope: Lift PDP and PFI to status of statistical estimator
- Needed: estimand. A definition of what it is supposed to estimate in the DGP (not the model)
- Idea: PDP and PFI are expectation of a function
- Assumption: True function $f$ exists in DGP
- Then: We can apply same interpretation tools to DGP. Very simple.

TODO: simple image

## Source of uncertainty

- The ground truth estimand allows use to do the usual statistical estimator stuff:
- For example: Measure MSE
- Or: split up MSE into bias (systematic deviation of estimator to estimand)
- Variance: Two sources in the estimation process:
  - Model is a random variable bc. training data is a sample + model-specific inherent randomness (such as subset sampling in random forests)
  - Expectations are based on Monte carlo sampling


## Model-PD and Model-PFI

Show definition plus confidence intervals plus image



## Learner-PD and Learner-PFI


Show definition plus confidence intervals plus image

But there is a problem when estimating this variance


## Variance correction

- Variance is underestimated as training data is shared by models.
- Same for test data.
- Underestimating variance also means confidence intervals are too short, appearing more certain than they really are.
- Same problem exists for variance estimates for model performance, as needed for comparing models
- Nadeau et. al (CITE) proposed correction term
- term very naive, but MUCH better than without correction

## Show simulation results


# Thank You for Your Attention





