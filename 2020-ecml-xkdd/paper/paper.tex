% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Interpretable Machine Learning -- State of the Art and Challenges\thanks{This project is funded by the Bavarian State Ministry of Science and the Arts and coordinated by the Bavarian Research Institute for Digital Transformation (bidt) and supported by the German Federal Ministry of Education and Research (BMBF) under Grant No. 01IS18036A.
The authors of this work take full responsibilities for its content.
}}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Christoph Molnar\inst{1}\orcidID{0000-0003-2331-868X}}
%\and
%Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
%Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
%
\authorrunning{Molnar}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{LMU Munich, Ludwigstr. 33, 80539 Munich, Germany
\email{christoph.molnar@gmail.com}\\
\url{https://www.slds.stat.uni-muenchen.de/people/molnar/}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
  I present a very short history of the field of interpretable machine learning (IML), give an overview of popular interpretation methods and discuss challenges when interpreting machine learning models.

\keywords{Interpretable Machine Learning \and Explainable AI}
\end{abstract}
%
%
%
Interpretability is often a deciding factor when machine learning (ML) is used in a product, a decision process or in research.
Interpretable Machine learning gives the means to debug a model, extract insights and justify decisions.
I want to give three very common use cases:

%\paragraph{For science:} In applied sciences, data is often modeled with ML nowadays, where before statistical modeling approach would have been used.
%The goal of science is not only to predict the world well, but also to generate further insights.
%Examples are species distribution models in ecology which aim to explain why certain species can be found on certain areas based on e.g. the local climate.
%Here, interpretability of the prediction model is needed to understand why a species, e.g., has a high probability to occur in a specific reason.


\paragraph{A Very Short History of IML.}
Learning interpretable prediction models from data is not new, but has been the focus of statistical learning and rule-based ML for a long time.
Statistics has a long tradition of learning interpretable prediction  models from data, starting in 1800 \cite{stigler1986history} with works on least squares by Legendre 1805, \cite{legendre1805nouvelles}, Gauss in 1809 \cite{gauss1809theoria} and Quetelet in 1827 \cite{quetelet1827recherches}.
Another methodological pillow are rule-based machine learning approaches, which includes models such as decision rules and decision trees.
  This took off in the 80s with decision tree approaches such as CART CITE, ID3 CITE and rule learners like AQ CITE but reaches back to first research in the 1960s \cite{hajek1966guha}.
Both research on linear regression models and its extensions and rule-based ML remain important and busy research areas to this day and are even blending together (e.g. model-based trees CITE).
As a next stepping stone to the field of IML are random forests, an early approach to explain more complex machine learning models.
I argue that one of the success factors of the random forest as a prediction model was (and is) the built-in feature importance measure.
The high citation counts of the original paper (CITE), but also suggestions for how to improve the feature importance (CITE, CITE, CITE) are evidence for this.
The IML field really took off in around 2015, judging on usage of the search terms "Interpretable Machine Learning" and "Explainable AI" on Google (see Figure XXX) and papers published with these terms (see Figure XXX).
Could it be that only the terms have changed, but the field has always been there and the same?
I would say no, since a lot of commonly used methods, especially model-agnostic ones and the ones for deep neural network also came about this time, see chapter (XXX).

\paragraph{Today} we have reached a first state of maturity where some methods were established, implemented in software and used by practitioners and applied researchers.
There is a lot of open source software that implement various techniques (CITE iml, CITE DALEX, CITE sklearn, ...).
Big tech companies offer software (CITE ai360, CITE what-if tool, ...).
Regulation such as GDPR has spurred a discussion around further needs of interpretability.
Many startup are now focused on explainable / interpretable ML (TODO: Find good source).


\section{Methods:}

In this section I want to very broadly talk about methods that we have to explain models.
A first useful distinction is between inherently interpretable models and usually model-agnostic explanation methods that describe the behavior of the model.
This is a distinction between trying to make sense of a model by analyzing its parts vs. making sense of it by analyzing its prediction for certain inputs (sensitivity analysis).
However we interpret a model, the interpretation is always projected back to the features themselves or some abstract transformation of them.

\paragraph{Inherently interpretable models} are models where we can interpret its learned structure and parameters.
In this category, I see all linear regression models of the form $Y = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$, where the weights $\beta$ are learned parameters of the model.
Through the models structure (weighted sum of features) we can interpret the weights as the effect that a feature has on the prediction.
Similarly, decision trees and other rule-based ML models have a learned structure which we can interpret as how the model makes a prediction.
For example, an individual prediction can be traced through a decision tree and we retrieve an explanation of the form "If feature $x_1$ larger than two and feature $x_4$ in category A, then prediction is 10".
However, the more complex these structures get (e.g., a linear model with hundres of features and complex interaction terms or very deep decision trees), the more difficult it becomes to see this as interpretable.
Surprisingly, we can also try assing meaning to structures of more complex models such as individual neurons or layers in a convolutional neural network.
This leads to the famous results where we find out that CNNs learn edge detectors at the lowest levels and increasingly abstract concepts at higher layers.

\paragraph{Sensitivity-analysis-like (model-agnostic) interpretation methods} often treats the model as a black-box (but not always).
Here I also want to distinguis between local and global explanations.

\paragraph{Local explanations} explain how individual predictions are derived.
Here we have a great variety of methods, with very different methodological motivations.
Popular methods are LIME, Shapley Values and Counterfactual Explanations.
I want to highlight especially their very different approaches to the problem of local explanations.
Counterfactual explanations are what-if scenarios.
How does an instance have to change to get another (desired) prediction or classification.
They build on a rich tradition in philosophy and also thrive in what the social sciences discovered:
Good explanations are contrastive and focus on a few reasons.
A very different approach originates from collaborative game theory: The Shapley Values.
How do you fairly share some payout from a game among the players?
The Shapley Values provide an answer for this question, by trying out all constellations of players and averaging the additional payout a player would add to the total payout when the player is added to a team.
The same idea can be applied to ML: The predicted value is the payout, the player is a certain feature value.Many variations, extensions and more efficient implementations for specific ML models exist (CITE shap, tree shap, Other stuff).
The third mention here is LIME.
LIME is a surrogate model, i.e., the procedure approximates the original ML model with a simpler interpretable model.
LIME does so locally, meaning only in some neighborhood around the data point that we want to explain.

\paragraph{Surrogate Models} are an interesting and huge class of explanation method.
This approach is very common in sensitivity analysis where we have emulation? models.
Surrogate models combine the two strategies:
First, approximate the predictions of the ML model with an interpretable model, such as a decision tree.
This is a kind of sensitivity-like approach, where we see the original model as a black box and artificially probe the ML model and derive information from the behavior.
Then we interpret the interpretable model, which now acts as a surrogate to the more complex ML model.


\paragraph{Global, model-agnostic explanation methods} are also very popular.
Here we have have feature importance, which rank features based on how relevant they were for the prediction and the feature effect, which expresses how a change in that feature changes the predicted outcome on average.
Permutation Feature Importance, which is based on permuting features is popular importance measure, originally suggested for random forests, now available as model-agnostic version.
Alternatives are variance based measures, see XXX for an overview of all the ways to measure importance.
For feature effects we have Partial Dependence Plots (CITE), Individual Conditional Expectation Plots (ICE) and Accumulated Local Effect (ALE) Plots (CITE), with of course many variations (CITE, CITE).





\section{old}


\paragraph{A Very Short History of Machine Learning Interpretability}
Learning interpretable prediction models from data is not new, but has been the focus of statistical learning and rule-based ML for a long time.
In statistics, there is a long tradition of learning interpretable models from data, starting in 1800 \cite{stigler1986history}.
The least squares  method to fit linear models reaches back to 1805 by Legendre. \cite{legendre1805nouvelles}
Later the linear regression model became a tool for astronomy and geodesy, e.g. by Gauss in 1809 \cite{gauss1809theoria}.
Starting 1827 Quetelet used linear regression models (with least squares method) for (at that time) large amounts of data from social sciences, such as crime data \cite{quetelet1827recherches}.
We can hardly call the least squares method interpretable machine learning, but it remains a highly relevant building block.
Another methodological pillow are rule-based machine learning approaches, which includes models such as decision rules and decision trees.
Until 1950s mostly statistical methods available, so I guess we can't speak yet of interpretable machine learning.
A side note: I don't want to distinguish too sharply between statistics and machine learning.
But for this context, I would say in statistical modeling the focus is on finding a suitable linear model using in-sample metrics such as R squared and AIC.
In machine learning we use an external test set to minimize the generalization loss and the model itself does not matter as long as the loss is low.

I want to highlight the random forest CITE as a representative for early interpretable ML approach, which also lay out some more groundwork and a new paradigm for feature importance.

In 2010s then came the deep learning hype.

I would still argue that the field has reached a new form, as evidenced by new keywords "Interpretable Machine Learning" and "Explainable Artificial Intelligence", but also by bringing in new ideas from other fields, pulling many unconnected research fields together and an additional focus on model-agnostic ML interpretation methods.

The big leap came in the mid 2010's, visible in Google Search trends [link] and Web of Science statistics when searching for 'Explainable AI' and 'Interpretable Machine Learning'.
These figures look like there has not been much research on explaining prediction models before, which would be a wrong conclusion.

Research was done, big time in statistics for regression models in its various extensions, rule based stuff in computer science, random forest and its many extensions and interpretation.
What has changed is that Interpretable Machine Learning or Explainable Artificial Intelligence has become a field that now ties all these topics together.
Especially model-agnostic methods that work for any type of model have grown a lot in this new field.

\paragraph{Where Do We Stand Today in 2020?}

We have software that implements interpretability (CITE lots of stuff here).
Many of the big IT companies
We have software, big companies that offer solutions and many startups.
Research is heavily underway and people are already using the solutions.

\paragraph{What Methods do We Have?}

There are many overview papers with categorizations of all these methods.
Here are the properties I find the most useful to categorize the various methods and allow a user to understand when to use which.

An interesting distinction is how the methods work:
They either explain individual 'parts' of the model, examples are the weights in a linear model, the structure of a tree.
This can be very straightforward such as the interpretation of a linear regression weight, which directly tells us how a feature influences the response.
Or this can be more complex, when we want to understand what the role of individual neurons in a convolutional neural network is.
Here we can do activation maximization which tells us to which image the neuron most strongly reacts to and interpret this neuron as, e.g., a detector of a certain pattern.
Methods that require to look at certain parts of the model are by definition model-specific, i.e. they cannot be applied to any model.
Methods that mostly observe the input - output behavior are often model-agnostic, i.e. they ignore whether the model is a neural network, linear model or random forest.

A second distinction is about whether a method explains an individual data point or the global model behavior.
Methods such as Shapley Values and LIME compute explanations for points, while there are many feature importance methods that quantify the importance of a feature globally for the model, meaning across all data points.

\subsection{Linear Models and Rule-based ML}

Linear models and rule can be seen as building blocks for many types of explanations and explanation methods.
Interpretations are often presented as a weighted sum of the inputs, such as LIME, Shapley Values, and many of the attribution methods for convolutional neural networks.
Even the entire model can -- in theory -- be deconstructed into and additive sum of components reaching from first-order effects to all interactions (cite functional ANOVA).
But also rule based explanations are very common types of explanations.
There are many new tree algorithms, such as XXX and XXX.

\paragraph{Linear Models}
The beauty of linear models is that they are additive, meaning that the influences of each feature on the target can be added together to give the full effect.
That makes estimation and interpretation simple.
But this is often not adequate since in the real world the effects are usually not linear.
Many extensions exists, such as non-linear transformations for the case that the target follows a different distribtuion (GLMs, logistic regression).
Other extensions allow for non-linear effect of a feature, by using splines.
Also interactions can be integrated.
All these extensions make the model more difficult to interpret, but also allow to more flexible model the target.

\paragraph{Rule-based ML}
There are different approaches.
They all have in common that they find decision rules in the form of conditions combined with AND conjunctions leading to some prediction if all conditions are matched.
There are different ways to find this rules and if they are allowed to overlap and so on.
Differences in how they are optimized and so on.


\subsection{Explain Individual Predictions}

There was a great explosion in different methods to explain individual predictions of ML models in a model-agnostic way.
Among the most popular are Local Interpretable Model-agnostic Explanations (LIME) CITE, Shapley Values from Game Theory (CITE x2) and Counterfactual Explanations.
Their outputs of LIME and Shapley Value are weighted sums and are estimated using method from linear regression, again showing the deep roots with statistics.
Counterfactual explanations are presented as decision rules, for LIME this is also possible

\paragraph{Surrogate Models}
Many methods -- a confusing amount, really -- are nothing but simple models that approximate the behavior of the (complex) model to be explained.

\paragraph{Counterfactual Explanations}
Counterfactual explanations are rather simple explanations for individual predictions.
A counterfactual explanation tells you how the features should be changed to get to a different prediction or classification.
But when you look closer at how you find them, it gets more difficult.
There are many tradeoffs to make.
First, you want to get as close as possible to the desired outcome.
At the same time you don't want to change too many features, and the features that are changed should not be changed too much.
Also these changes should not create unrealistic combinations of feature values, think of a 2m person for which the weight was changed to 30kg.
The reason that there are many papers on the topic is that the search for counterfatuals can be expensive.
Some methods are only for specific types of models, and they all trade-off the various objectives in different ways.
We argued that it is necessary to treat this as a multi-objective problem and the result should be not one but many counterfactual explanations.

\paragraph{LIME}

\paragraph{Shapley Values}


\subsection{Global Model Interpretation}

\paragraph{Feature Effects}

\paragraph{Feature Importance}


\subsection{Other Methods}

\paragraph{Heatmaps}

\section{Challenges}

\subsection{Dependent Features}
Feature dependence comes in many forms, correlation being the simplest and most known.
When features are dependent -- and they usually are -- disentangling the effect of one from the other are difficult if not impossible.
For example, if you measure various markers of inflammation in a blood sample, they will be highly correlated.
Using these markers in a predictive model for classifying diseases for which these markers are highly predictive, the model can rely on any of them with arbitrary weighting.
Attributing which marker was the most important now becomes difficult.

Almost all interpretation methods are affected by this issue.
Most of them ignore this issue and pretend the features are independent of each other.
For example the permutation feature importance measure quantifies the importance as the drop in performance when the feature is permuted.
But permutation when features are dependent not only breaks the association with the target value, but also with the other features.
Now if we permute one blood marker, it might create new combination that might be very unlikely.
These combinations are now fed to the model to get the predictions, but these are outside of the training data distribution and the model never had to deal with similar combinations.
It is totally unclear how the model will behave and it might even be irrelevant how the model will behave for these data points as they will never occur in real life.
There are attempts to 'fix' this issue, namely to not permute the feature, but to draw from the conditional distribution.
This means we still sample new values, but only the ones that are conforming with the other feature values.
Unfortunately, this is not a simple 'fix' of the issue.
While it solves the extrapolation issue, it becomes a new measure altogether, since conditional feature importance measure the drop in performance "given that we know the other feature values".
This can mean that the two most important features can drop to almost zero importance when yhey are highly correlated.

Similar things happen to 'fixes' for e.g. Shapley Values.
Once we start sampling from the conditional distribution and not from the marginal, we don't get Shapley Values for our original question, but something else.

I fear that there is no easy solution.


\subsection{Causality}
In general, we are not allowed to make a causal interpretation of machine learning models.
There is an inherent conflict between prediction performance optimization and causal modeling.
A feature might give some additional predictive performance, but its effect may not be interpreted causally, because, e.g., we did not include the confounder of both this feature and the target.
The fact that the ground has some predictive value for tomorrow's weather, but it's not causal for it.
The common confounder is today's weather -- todays rain makes the ground wet.
And if we had no access to todays weather but only to the "wet ground"-feature, we have to decide between predictive performance and causal correct interpretation.

\subsection{Inference}
Example for confidence intervals for LRP https://arxiv.org/pdf/2008.01468.pdf.
Is model-specific for Deep Neural Networks using Dropout.

\subsection{Many More Topics}
This was not an exhaustive list, and especially focused on the methodological side, neglecting views of the user interface, societal impacts, psychological insights and so on.
These challenges are more from the methodological, mathematical view, i.e., what can be done given we have some data and model.
This is also a rather static view point in the sense that we can do a lot if we allow a more interactive way of modeling and "having a conversation" between a person and the model or even the modeling process.

\section{Discussion}

While Interpretable Machine Learning can seem like a pretty young field, it has some old roots in Statistics and Computer Science.
Yet, many new methods have come in the recent years and we have witnessed a Cambrian explosion of methods, software and institutional interest.
We are seing both ways of making models themselves more interpretable, but also model-agnostic methods that work for any type of model.
These methods also are implemented now in many software packages.
Big leaps in the field came from old ideas in other fields.
The Shapley Value has been for a long time part of game theory, and its introduction to machine learning interpretability has spurred a lot of development.
We can learn a lot from Statistics and how it handles uncertainty.

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliography{mybibliography}
%

\vskip 0.2in
\bibliographystyle{splncs04}
\bibliography{Bib}

\end{document}
