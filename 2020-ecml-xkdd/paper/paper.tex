% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Interpretable Machine Learning -- A Short History, State of the Art and Challenges\thanks{This project is funded by the Bavarian State Ministry of Science and the Arts and coordinated by the Bavarian Research Institute for Digital Transformation (bidt) and supported by the German Federal Ministry of Education and Research (BMBF) under Grant No. 01IS18036A.
The authors of this work take full responsibilities for its content.
}}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Christoph Molnar\inst{1}\orcidID{0000-0003-2331-868X}}
%\and
%Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
%Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
%
\authorrunning{Molnar}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{LMU Munich, Ludwigstr. 33, 80539 Munich, Germany
\email{christoph.molnar@gmail.com}\\
\url{https://www.slds.stat.uni-muenchen.de/people/molnar/}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
  I present a very short history of the field of interpretable machine learning (IML), give an overview of popular interpretation methods and discuss challenges when interpreting machine learning models.

\keywords{Interpretable Machine Learning \and Explainable AI}
\end{abstract}
%
%
%
Interpretability is often a deciding factor when machine learning (ML) is used in a product, a decision process or in research.
Interpretable Machine learning gives the means to debug a model, extract insights and justify decisions.
I want to give three very common use cases:

%\paragraph{For science:} In applied sciences, data is often modeled with ML nowadays, where before statistical modeling approach would have been used.
%The goal of science is not only to predict the world well, but also to generate further insights.
%Examples are species distribution models in ecology which aim to explain why certain species can be found on certain areas based on e.g. the local climate.
%Here, interpretability of the prediction model is needed to understand why a species, e.g., has a high probability to occur in a specific reason.

TODO: Mention sensitivity analysis


\paragraph{A Very Short History of IML.}
Learning interpretable prediction models from data is not new, but has been the focus of statistical learning and rule-based ML for a long time.
Statistics has a long tradition of learning interpretable prediction  models from data, starting in 1800 \cite{stigler1986history} with works on least squares by Legendre 1805, \cite{legendre1805nouvelles}, Gauss in 1809 \cite{gauss1809theoria} and Quetelet in 1827 \cite{quetelet1827recherches}.
Another methodological pillow are rule-based machine learning approaches, which includes models such as decision rules and decision trees.
Many extensions exists, such as non-linear transformations for the case that the target follows a different distribtuion (GLMs, logistic regression).
The beauty of linear models is that they are additive, meaning that the influences of each feature on the target can be added together to give the full effect.
  This took off in the 80s with decision tree approaches such as CART and ID3 and rule learners like AQ \cite{furnkranz2012foundations} but reaches back to first research in the 1960s \cite{hajek1966guha}.
Both research on linear regression models and its extensions and rule-based ML remain important and busy research areas to this day and are even blending together (e.g. model-based trees \cite{zeileis2008model}).
As a next stepping stone to the field of IML are random forests, an early approach to explain more complex machine learning models.
I argue that one of the success factors of the random forest as a prediction model was (and is) the built-in feature importance measure.
The high citation counts (>60000 citation on Google Scholar as of September 2020) of the original paper \cite{breiman2001random}, but also suggestions for how to improve the random forest feature importance (\cite{strobl2008conditional,strobl2007bias,hapfelmeier2014new,ishwaran2007variable}) are evidence for this.

In 2010s then came the deep learning hype.
And a few years after that, the IML field really took off in around 2015, judging on usage of the search terms "Interpretable Machine Learning" and "Explainable AI" on Google (Figure \ref{fig:count} right)and papers published with these terms (Figure \ref{fig:count} left).
\begin{figure}
  \includegraphics[width=\textwidth]{citation-search.png}
  \caption{Left, Right}
  \label{fig:count}
\end{figure}
Could it be that only the terms have changed, but the field has always been there and the same?
I would still argue that the field has reached a new form, as evidenced by new keywords "Interpretable Machine Learning" and "Explainable Artificial Intelligence", but also by bringing in new ideas from other fields, pulling many unconnected research fields together and an additional focus on model-agnostic ML interpretation methods.
I would say no, since a lot of commonly used methods, especially model-agnostic ones and the ones for deep neural network also came about this time.
These figures look like there has not been much research on explaining prediction models before, which would be a wrong conclusion, since developments happened under different names and in different fields.
What has changed is that Interpretable Machine Learning or Explainable Artificial Intelligence has become a field that now ties all these topics together.


\paragraph{Today} we have reached a first state of maturity where some methods were established, implemented in software and used by practitioners and applied researchers.
There is a lot of open source software that implement various techniques (\cite{iml,biecek2018dalex,pedregosa2011scikit,klaise2020alibi,nori2019interpretml}, ...).
Big tech companies offer software \cite{exler2019if,arya2020ai,hall2017machine}...
Regulation such as GDPR has spurred a discussion around further needs of interpretability.
Many startup are now focused on explainable / interpretable ML (TODO: Find good source).


\section{Methods:}

In this section I want to very broadly talk about methods that we have to explain models.
However we interpret a model, the interpretation is always projected back to the features themselves or some abstract transformation of them.
Often the distinction is made between model-specific (works only for one type of model) and model-agnostic (works for all models) interpretation methods.
I want to distinguish between introspection and sensitivity analysis.
An interpretation method that works by introspection assigns semantic meaning to \textit{(learned) model parameters and structures} while sensitivity analysis \textit{probes the model and describes its behavior}.
Introspection is necessarily model-specific.
Sensitivity analysis is mostly model-agnostic, but can also be done by leveraging model-specific knowledge such as gradient-based methods. FOOTNOTE: An example are the various heatmap-like explanation for CNNs for image classification \cite{sundararajan2017axiomatic,lundberg2017unified,montavon2017explaining,simonyan2013deep,shrikumar2016not} which study the sensitivity of the prediction to the input pixels. Technically this is done by backpropagation and therefore model-specific. Model-agnostic versions \cite{ribeiro2016should,lundberg2017unified,zeiler2014visualizing} exist.
The other useful category is local vs. global explanations.
Local means that the method explains individual predictions, and global means it explains the average model behavior.
For inherently interpretable models, global and local explanations are often the same: In linear models, the coefficients both how the model behaves globally, but also is used for individuals instances.

\paragraph{Inherently interpretable models} are models where we can interpret its learned structure and parameters.
As a rule of thumb for when we do an introspection is that we are not probing the model with various (manipulated data points).
In this category, I see all linear regression models of the form $Y = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p$, where the weights $\beta$ are learned parameters of the model.
Through the models structure (weighted sum of features) we can interpret the weights as the effect that a feature has on the prediction.
Similarly, decision trees and other rule-based ML models have a learned structure which we can interpret as how the model makes a prediction.
For example, an individual prediction can be traced through a decision tree and we retrieve an explanation of the form "If feature $x_1$ larger than two and feature $x_4$ in category A, then prediction is 10".
However, the more complex these structures get (e.g., a linear model with hundreds of features and complex interaction terms or very deep decision trees), the more difficult it becomes to see this as interpretable.
Surprisingly, we can also try assign meaning to structures of more complex models such as individual neurons or layers in a convolutional neural network.
This leads to the famous results where we find out that CNNs learn edge detectors at the lowest levels and increasingly abstract concepts at higher layers.
Another example is the random forest minimal depth distribution:\cite{randomForestExplainer}
The distribution of minimal depth shows for a feature for all the trees in the forest at which depth it occurs, with the notation that it the feature is more important if it appears early in the tree, it is more important.
This is again an example how we analyze the structure of a model to get an understanding of how it makes decisions, but the model is more complex (hundreds of decision trees) so there is more work to interpreting its learned structure.
The random forest is also a useful example for another measure, the feature importance, for which various measures exist, such as decrease in accuracy and decrease in Gini impurity.
By using our distinction of introspection and sensitivity analysis Gini impurity would fall into the former while mean decrease accuracy falls into the latter.
Gini importance collects for each feature and tree how the split decreased the Gini impurity.
It does not rely on making predictions, but only on internal statistics.
For mean decrease accuracy, we probe the random forest with perturbed data, i.e., we shuffle a feature and see how the prediction changes.
It's a model-specific methods since we do this tree-wise and use the out-of-bag sample of the tree, so this procedure is cleverly adapted to the random forest.
Model-agnostic versions of this permutation feature importance exists \cite{fisher2019all}.

\paragraph{Sensitivity-analysis-like (model-agnostic) interpretation methods} often treats the model as a black-box (but not always).
Here I also want to distinguish between local and global explanations.

\paragraph{Local explanations} explain how individual predictions are derived.
Here we have a great variety of methods, with very different methodological motivations.
Popular methods are LIME, Shapley Values and Counterfactual Explanations.
I want to highlight especially their very different approaches to the problem of local explanations.
Counterfactual explanations are what-if scenarios.
How does an instance have to change to get another (desired) prediction or classification.
They build on a rich tradition in philosophy and also thrive in what the social sciences discovered:
Good explanations are contrastive and focus on a few reasons.
A very different approach originates from collaborative game theory: The Shapley Values.
How do you fairly share some payout from a game among the players?
The Shapley Values provide an answer for this question, by trying out all constellations of players and averaging the additional payout a player would add to the total payout when the player is added to a team.
The same idea can be applied to ML: The predicted value is the payout, the player is a certain feature value.Many variations, extensions and more efficient implementations for specific ML models exist (CITE shap, tree shap, Other stuff).
The third mention here is LIME.
LIME is a surrogate model, i.e., the procedure approximates the original ML model with a simpler interpretable model.
LIME does so locally, meaning only in some neighborhood around the data point that we want to explain.

\paragraph{Surrogate Models} are an interesting and huge class of explanation method.
This approach is very common in sensitivity analysis where we have emulation? models.
Many methods -- a confusing amount, really -- are nothing but simple models that approximate the behavior of the (complex) model to be explained.
Surrogate models combine the two strategies:
First, approximate the predictions of the ML model with an interpretable model, such as a decision tree.
This is a kind of sensitivity-like approach, where we see the original model as a black box and artificially probe the ML model and derive information from the behavior.
Then we interpret the interpretable model, which now acts as a surrogate to the more complex ML model.


\paragraph{Global, model-agnostic explanation methods} are also very popular.
Here we have have feature importance, which rank features based on how relevant they were for the prediction and the feature effect, which expresses how a change in that feature changes the predicted outcome on average.
Permutation Feature Importance, which is based on permuting features is popular importance measure, originally suggested for random forests, now available as model-agnostic version.
Alternatives are variance based measures, see XXX for an overview of all the ways to measure importance.
For feature effects we have Partial Dependence Plots (CITE), Individual Conditional Expectation Plots (ICE) and Accumulated Local Effect (ALE) Plots (CITE), with of course many variations (CITE, CITE).


\section{Challenges}

There are also challenges for the field, that are already being tackled but still remain issues.
This list is also based on this paper (CITE Pitfalls).

\paragraph{Feature dependence} makes interpretation more difficult.
Inherently, when two features are dependent, e.g., correlated, attributing effects and importance to them is difficult.
Most methods also suffer from extrapolation when features are dependent.
When features are, e.g., permuted individually, this breaks the association with the target to predict (that is desirable) but it also breaks the association with the other features (which can cause problems).
The problem here is that new data points are being generated that are unlikely.
These points are then used for computing importance values, explanations for individual predictions and so on.
Many model-agnostic methods have this issue: PFI, PDP, Shapley values, LIME, ...
But also interpretation for inherently interpretable models can suffer:
When features are strongly correlated, the variance of estimated weights in a linear model can become large.
Also the interpretation of a weight is conditional that all other features remain fixed, which becomes the more unrealistic the more dependence there is between features.
There are some "fixes" for it, which enforce that any manipulation of the data respects the joint distribution.
All solutions have the problem that they now mix the effects and importance of features that switch the focus of interpretation or even break the original interpretation method in the case of Shapley values (CITE, CITE).
Solutions exist for feature importance, called conditional feature importance CITE APPROACHES.
Currently there is no satisfying solution that fixes extrapolation, but keeps the original marginal interpretation.
I fear that there is no easy solution.

\paragraph{Inference}
In statistics it is common procedure to quantify the uncertainties that come with estimates of, e.g., coefficients in a linear model.
Quantification of uncertainty, be it in the form of error bars, confidence intervals, test statistics or p-values, helps to separate signal from noise.
Many techniques in IML currently only give the point estimates, but don't quantify the uncertainty.
For things like feature importance there are variations, but we need to have these things built into common software tools.
Also it is unclear which uncertainty should be quantified and for what purpose.
Should we treat the model as fixed?
Or should we quantify the uncertainty of some feature effect in the same manner as we do for the generalization error, that is with for example cross-validation.
Here we have to dig deeper into the statistics literature which can be our guide.
Example for confidence intervals for LRP https://arxiv.org/pdf/2008.01468.pdf.
Is model-specific for Deep Neural Networks using Dropout.



\paragraph{Causal interpretation}
Usually, we desire a causal interpretation of an effect or importance.
As a researcher you want to draw conclusion about the true underlying process that generated your observations and learn about the world.
When you want to use ML for decision making, you need causal reasons that can change the target in the real world.
For example, when modeling the likelihood that customers will quit their subscription, only a good prediction will not be enough, but you have the desire to prevent a customer from leaving.
Only causal interpretation of the model will help you find reasonable real-world interventions.
However, this is generally not possible and causality can even be a conflicting goal to predictive accuracy.
For example when we don't have access to an important confounder of a (non-causal) feature and the target, it improve predictive performance if we include the non-causal feature.
When we want to predict whether it will rain tomorrow, a useful feature might be whether the ground is wet today, assuming we don't have access to today's weather.
Todays weather is the cause for both the wet ground and tomorrows weather.
Including it in the model will improve performance, but we are not allowed to interpret the effect and importance for the wet ground causally for the weather tomorrow.
Here, knowledge from the causality community is important.
It's not enough to somehow fix causality within the interpretation method, but we must go back to the modeling process itself to consider causality already there.

\paragraph{Lack of definition of interpretability and explainability} is a common critique of the field \cite{lipton2018mythos}.
First, I don't think the lack of definition is a problem for the "naming" of the field.
I think of "Intepretable ML" and "Explainable AI" as useful terms to collect all topics that are related to analyzing ML models beyond performance metrics.
There is also very valid critique in lack of definition.
Because there is no single, clean definition of interpretability, quantitative research results can be hard to come by, e.g., how can you show that some interpretation method delivers better explanations than another?
This is connected to a lack of groundtruth, we just don't know what the correct explanation is.
To counter this argument, interpretability depends on audience and context TODO:CITE.
It seems like we are on our way to have many definitions of \textit{various aspects} of interpretability such as sparsity, interaction strength, simulability (CHECK) and much more \cite{poursabzi2018manipulating,philipp2018measuring,molnar2019quantifying,hauenstein2018computing,zhou2018measuring,akaike1998information,schwarz1978estimating,poursabzi2018manipulating,dhurandhar2017tip,friedler2019assessing},
It will take some time to establish some best practice for evaluation interpretation methods and the explanations they produce.

\paragraph{And much more} research has to be done on which I did not touch.
This was not an exhaustive list, and especially focused on the methodological side, neglecting views of the user interface, societal impacts, psychological insights and so on.
These challenges are more from the methodological, mathematical view, i.e., what can be done given we have some data and model.
This is also a rather static view point in the sense that we can do a lot if we allow a more interactive way of modeling and "having a conversation" between a person and the model or even the modeling process.
How do we present multiple maybe conflicting explanations, e.g., when you generate multiple counterfactual explanations?

\section{Discussion}

While Interpretable Machine Learning can seem like a pretty young field, it has some old roots in Statistics and Computer Science.
Yet, many new methods have come in the recent years and we have witnessed a Cambrian explosion of methods, software and institutional interest.
We are seeing both ways of making models themselves more interpretable, but also model-agnostic methods that work for any type of model.
These methods also are implemented now in many software packages.
Big leaps in the field came from old ideas in other fields.
The Shapley Value has been for a long time part of game theory, and its introduction to machine learning interpretability has spurred a lot of development.
We can learn a lot from Statistics and how it handles uncertainty.

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliography{mybibliography}
%

\vskip 0.2in
\bibliographystyle{splncs04}
\bibliography{Bib}

\end{document}
