---
title: "Interpretable Machine Learning"
author: "Christoph Molnar"
date: "June 7, 2018"
output: 
  ioslides_presentation:
    widescreen: false
    smaller: false
css: ../styles.css
---


```{r, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, 
  warning = FALSE,
  message = FALSE, 
  fig.align='center', 
  out.width = '100%')
```


```{r, child="../chunks/story-salaries.Rmd"}
```


# Some Theory


## The Problem 

```{r, echo=FALSE, fig.align='center'}
knitr::include_graphics("images/black-box.png")
```
<div class="notes">
- Opaque decision making by machine learning algorithms
- Bias and mistakes in the data
- Trust as a user: Should I start a therapy with sever side effects, because the machine said so?
- Debugging as a practitioner: Why did the algorithm miss-classify sample X? Did it learn generalizable features? 
</div>


# When do we need interpretability?


## {.center data-background=images/credit.jpg data-background-size=cover}

## {.center data-background=images/pill.jpg data-background-size=cover}

## {.center data-background=images/death.jpg data-background-size=cover}

## {.center data-background=images/people.jpg data-background-size=cover}


<div class="notes">
We need interpretability when the loss function does not cover all constraints. 

- Decisions about humans.
- Critical applications that decide about life and death.
- Newly developed systems with unknown consequences.
- Models using proxies instead of causal inputs.
- Debugging the models.
- Increasing trust.
</div>



# When do we NOT need interpretability?


##  {.center data-background=images/mnist.jpeg data-background-size=cover}

<div class="notes">
When we can capture everything in the loss function and the data collection.
only causal relationships.
perfect operationalization of features.

Things that work well. 
Well defined problems
</div>

```{r child = "../chunks/tools.Rmd"}
```


```{r child = "../chunks/book.Rmd"}
```

# Backup slides

# What is interpretability?

<div class="notes">
Interpretability is the degree to which a human can understand the cause of a decision. 
But we don't really have a good way to measure that really. Not as easy as benchmarking ML algorithms. 

Interpretability is also a mean to look at the data and possible issues with them.

<font size="2">Miller, Tim. 2017. “Explanation in Artificial Intelligence: Insights from the Social Sciences.” arXiv Preprint arXiv:1706.07269.</font>
</div>

# Statistical Modeling: The Two Cultures

<div class="notes">
Paper by Leo Breiman (Random Forest)
data modeling culture (statistics) vs algorithmic modeling culture (ml)
tradeoff between interpretability and predictive performance
example: tree vs. random forest, logistic regression vs. neural network
</div>

# What makes an explanation human-friendly?

<div class="notes">
short: 1-2 causes
counterfactual: compares to other output
truthful
social context
consistent with prior beliefs
probable
focuses on abnormal causes
</div>


# Shapley Value

## {.center data-background=images/team.jpg data-background-size=contain}


## 
```{r}
set.seed(44)
```


```{r}
explanation = Shapley$new(predictor)
explanation$explain(X[15, ])
explanation$plot()
```
